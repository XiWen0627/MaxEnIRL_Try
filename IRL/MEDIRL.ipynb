{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec130a9-5e1a-40bf-915d-9d20237dac9b",
   "metadata": {},
   "source": [
    "# Step 1 Python Dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192bfe5-e812-4d15-a86d-4d5e4b050d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import product\n",
    "from gridWalk import bicycleGridRiding\n",
    "from DPforGridBike_v1 import ValueIteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cae64e-bca0-4943-bd6f-3f80ba264588",
   "metadata": {},
   "source": [
    "Python Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6175b97c-ad94-4e4f-a9fa-a05ca3404083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置 numpy 科学计数法\n",
    "np.set_printoptions(suppress=True, threshold=5000)\n",
    "# 设置 pandas 的格式\n",
    "pd.set_option('display.max_columns', None)  # 显示所有列\n",
    "pd.set_option('display.float_format', lambda x: '%.8f' % x)\n",
    "# Full Tensor\n",
    "torch.set_printoptions(sci_mode=False, profile=\"full\")\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b90c9f-080c-4f96-a299-5c400ffa3aa3",
   "metadata": {},
   "source": [
    "### 设置随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e671bf-08c5-4fd9-8afb-a248a70d70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0c393-2174-4f23-9889-a802dd20aa47",
   "metadata": {},
   "source": [
    "# Step 2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a562f8-2471-4c87-b884-8a85cccdc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajProccess():\n",
    "    def __init__(self, env, trajData, device):\n",
    "        self.trajData = trajData\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "\n",
    "    @ property\n",
    "    def maxTrajLength(self):\n",
    "        '''\n",
    "        Max Length For Given Trajectories\n",
    "        :param trajData: Dataframe, Given Trajectories\n",
    "        '''\n",
    "        trajIDs = np.unique(self.trajData['TrajID'])\n",
    "\n",
    "        trajectoryList = []\n",
    "        trajectoryLength = []\n",
    "        for trajID in trajIDs:\n",
    "            trajectory = []\n",
    "            trajID_Data = self.trajData[self.trajData['TrajID'] == trajID].copy()  # The Information of Current Trajectory\n",
    "\n",
    "            # [state, action, nextState]\n",
    "            trajID_Data.apply(lambda x: trajectory.append([x['state'], x['action'], x['nextState']]), axis=1)\n",
    "\n",
    "            trajectoryList.append(trajectory)  # Current Trajectory\n",
    "            trajectoryLength.append(len(trajectory))\n",
    "\n",
    "        return trajectoryList, max(trajectoryLength)\n",
    "\n",
    "    @property\n",
    "    def Padding(self):\n",
    "        '''\n",
    "        Trajectories Padding Function\n",
    "        :param trajectories:\n",
    "        :param maxTL: Max Length Of Given Trajectories\n",
    "        :return: trajMatrix, size = [numTraj, maxTL]\n",
    "        '''\n",
    "        trajectories = self.maxTrajLength[0]\n",
    "        maxLength = self.maxTrajLength[1]\n",
    "\n",
    "        trajectoryMatrix = []\n",
    "        for trajectory in trajectories:\n",
    "            if len(trajectory) != maxLength:\n",
    "                for i in range((maxLength - len(trajectory))):\n",
    "                    trajectory.append([-1, -1, -1])\n",
    "                trajectoryMatrix.append(trajectory)\n",
    "            else:\n",
    "                trajectoryMatrix.append(trajectory)\n",
    "\n",
    "        return torch.tensor(trajectoryMatrix, dtype=torch.float32).to(self.device)\n",
    "\n",
    "    def find_svf(self):\n",
    "        # print('Finding SVF ···')\n",
    "        numStates = len(self.env.states)    # numStates\n",
    "        # trajectoryList = self.Padding()    # []\n",
    "        trajectoryList = self.maxTrajLength[0]    # []\n",
    "        svf = np.zeros(numStates)  # [numStates, 1]\n",
    "\n",
    "        for trajectory in trajectoryList:\n",
    "            # print('轨迹为{}'.format(trajectory))\n",
    "            for stateActionPair in trajectory:\n",
    "                # [1373.0, 0.0, 1373.0]\n",
    "                state = int(stateActionPair[0])\n",
    "                svf[state] += 1\n",
    "                # if state != -1:\n",
    "                #     svf[state] += 1\n",
    "\n",
    "        # svf /= trajectories.shape[0]\n",
    "        # [[s,a], [s,a],[s,a]],\n",
    "        # [[s,a], [s,a],[s,a]]\n",
    "        svf /= len(trajectoryList)\n",
    "\n",
    "        a = torch.tensor(svf, dtype=torch.float32).to(self.device)\n",
    "        a1 = F.normalize(a, dim=0)\n",
    "        \n",
    "        return a1\n",
    "\n",
    "    def find_expected_svf(self, r=None):\n",
    "        # print('Finding Expected SVF ······')\n",
    "        env = self.env\n",
    "\n",
    "        # trajMatrix.shape -> [21237, 120, 3]\n",
    "        trajMatrix = self.Padding    # [numTraj, maxLength, 3]\n",
    "        n_trajectories = trajMatrix.shape[0]  # [numTrajectories, 1]\n",
    "        trajectory_length = trajMatrix.shape[1]  # [trajectoryLength, 1]\n",
    "\n",
    "        # ########### 修改了此处传参的过程 ###########\n",
    "        # if r != None:\n",
    "        #     agent.rewardFunc = r.cpu().detach().numpy()    # [numStates, 1]\n",
    "        # else:\n",
    "        #     agent.rewardFunc = [env.rewardFunc(agentloc) for agentloc in range(len(env.states))]    # 此处传参的过程可能存在问题\n",
    "            \n",
    "        ######## 5月11日修改结果  ######\n",
    "        if r != None:\n",
    "            r1 = r.cpu().detach().numpy()    # [numStates, 1]\n",
    "        else:\n",
    "            r1 = [env.rewardFunc(agentloc) for agentloc in range(len(env.states))]    # 此处传参的过程可能存在问题\n",
    "\n",
    "        agent = ValueIteration(env, r1)\n",
    "        GridV = agent.plan(threshold=0.01)\n",
    "        policy = agent.get_policy(GridV)\n",
    "        # Reward Function Is Not Available - Why ?    -> 没有将奖励函数传入至 Value Iteration 之中\n",
    "\n",
    "        start_state_count = torch.zeros(len(env.states), dtype=torch.float32)  # [numStates, 1]\n",
    "        for trajectory in trajMatrix:\n",
    "            start_state_count[int(trajectory[0, 0])] += 1\n",
    "        p_start_state = start_state_count / n_trajectories  # the Probability of Start State\n",
    "\n",
    "        expected_svf = torch.tile(p_start_state, (trajectory_length, 1)).T    # [trajLength, numStates] -> [numStates, trajLength]\n",
    "        for t in range(1, trajectory_length):\n",
    "            expected_svf[:, t] = 0  # Trajectory 长度为 trajLength, 初始化为起始点出现的概率\n",
    "            for i, j in product(range(len(env.states)), range(env.action_space.n)):\n",
    "                action = env.index2Action(j)\n",
    "                transitionProb = env.transitFunc(i, action)    # dict\n",
    "\n",
    "                # Next State\n",
    "                for k in transitionProb.keys():\n",
    "                    expected_svf[k, t] += (expected_svf[i, t - 1] *\n",
    "                                           policy[i, j] *  # Stochastic policy\n",
    "                                           transitionProb[k])\n",
    "            print('第{}个位置已经完成'.format(t))\n",
    "            \n",
    "        b = expected_svf.sum(dim=1, dtype=torch.float32).to(self.device)  # [numStates, 1]\n",
    "        b1 = F.normalize(b, dim=0)\n",
    "        \n",
    "        return b1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031a63c8-d53c-45e1-9a0e-1921e48a7856",
   "metadata": {},
   "source": [
    "图卷积得到的 Reward 传入矩阵之中，不存在道路的 reward 强制修改为某个特定的常数，以得到智能体环境的高效表示，避免噪音的影响；\n",
    "- 这种情况下卷积神经网络能否得到相似的结果？\n",
    "- 首先尝试卷积神经网络作为依赖学习的方式\n",
    "- 似乎不需要将图卷积神经网络加入到函数的设计之中"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb79f711-cf9f-46d3-bc8c-b6d06f14eb22",
   "metadata": {},
   "source": [
    "更新日志\n",
    "- 2024.5.18 9：05 按照上述的思路，在不加入卷积神经网络或者是图神经网络的基础上，学习其奖励以明确这种方式是否可以收敛 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc266b8-eec0-4c01-bbc3-6600bd7d345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepIRL():\n",
    "    def __init__(self, env, stateFeatures, trajProcess,\n",
    "                 ini_net, discount, device, epochs, passList,\n",
    "                 learning_rate=0.1, l1=0.1, l2=0.1):\n",
    "        self.env = env\n",
    "        self.stateFeatures = stateFeatures\n",
    "        self.passList = passList\n",
    "        self.trajProcess = trajProcess\n",
    "        self.ini_net = ini_net\n",
    "        self.discount = discount\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.device = device\n",
    "\n",
    "    def update(self, net, alpha):\n",
    "        # Forward Propagation\n",
    "        # print('Forward Propagation ····· ')\n",
    "        n_states, d_states = self.stateFeatures.shape  # numStates, dimension\n",
    "        reward = torch.matmul(net(self.stateFeatures), alpha).reshape(n_states,).to(self.device)    # [numStates, dimensions]\n",
    "        reward = (reward - reward.mean()) / reward.std()    # Reward Standardization\n",
    "\n",
    "        # 2024.5.18 日晚更新：路径规划器仍然需要获得所有的奖励\n",
    "        # 加入一个变量为 reward_temp 保存暂时的奖励\n",
    "        reward_temp = torch.zeros(len(self.env.states))\n",
    "        # passList 应该为一个可以通过的 FID 列表\n",
    "        # rewardTemp 应该是一个与环境大小相同的网格单元\n",
    "        i = 0\n",
    "        for j in self.passList:\n",
    "            reward_temp[j] = reward[i]\n",
    "            i += 1\n",
    "        \n",
    "        # print(reward)\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for name, param in net.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                weights.append(param)\n",
    "            elif 'bias' in name:\n",
    "                biases.append(param)\n",
    "\n",
    "        # print('Back Propagation ······')\n",
    "        adagrad_epsilon = 1e-6  # AdaGrad numerical stability.\n",
    "        expected_svf = trajProcess.find_expected_svf(r=reward_temp).reshape(len(self.env.states),)  # torch.tensor, size=[numStates, 1] -> After Normalization\n",
    "        svf = trajProcess.find_svf().reshape(len(self.env.states),)  # torch.tensor, size=[numStates, 1] -> After Normalization\n",
    "        grad = svf - expected_svf  # [numStates, 1]\n",
    "        grad1 = torch.Tensor([grad[s] for s in self.passList]).to(self.device)    # n_states\n",
    "\n",
    "        updates = []\n",
    "        hist_alpha_grad = torch.zeros(alpha.shape).to(self.device)  # [dimensions, 1]\n",
    "        output = net(self.stateFeatures)    # [numStates, output_dim] -> [numStates, dimension] -> [numStates, 4]\n",
    "        alpha_grad = (torch.matmul(grad1.T, output).\n",
    "                      reshape(alpha.shape))  # [output_dim, 1] -> [dimensions, 1] -> [4, 1]\n",
    "        hist_alpha_grad += alpha_grad ** 2  # history grad    [output_dim, 1] -> [dimensions, 1]\n",
    "        adj_alpha_grad = alpha_grad / (adagrad_epsilon + torch.sqrt(hist_alpha_grad))\n",
    "        updates.append((alpha, alpha + adj_alpha_grad * self.learning_rate))\n",
    "\n",
    "\n",
    "        def grad_for_state(s, theta, svf_diff, r):\n",
    "            \"\"\"\n",
    "            Calculate the gradient with respect to theta for one state.\n",
    "            \"\"\"\n",
    "            regularisation = torch.sum(torch.abs(theta)) * self.l1 + torch.sum(theta ** 2) * self.l2\n",
    "            autograd = torch.autograd.grad(r[s], theta, retain_graph=True)[0]\n",
    "            svf_diff_s = svf_diff[s]\n",
    "            return  svf_diff_s * autograd - regularisation\n",
    "            \n",
    "\n",
    "        hist_w_grads = [torch.zeros_like(weight).to(self.device) for weight in weights]\n",
    "        for i, W in enumerate(weights):\n",
    "            w_gradList = []\n",
    "            for state in range(n_states):\n",
    "                w_template_grad = grad_for_state(state, W, grad1, reward)\n",
    "                w_gradList.append(w_template_grad)\n",
    "            \n",
    "            # 计算梯度\n",
    "            w_grads = torch.stack(w_gradList)\n",
    "            w_grad = torch.sum(w_grads, dim=0)\n",
    "            # 清零历史梯度\n",
    "            if hist_w_grads[i] is not None:\n",
    "                hist_w_grads[i].zero_()\n",
    "            # 更新历史梯度平方累积和\n",
    "            hist_w_grads[i] += w_grad ** 2\n",
    "            # 计算调整后的梯度\n",
    "            adj_w_grad = w_grad / (adagrad_epsilon + torch.sqrt(hist_w_grads[i]))\n",
    "            # 添加参数更新规则到更新列表\n",
    "            updates.append((W, W + adj_w_grad * self.learning_rate))\n",
    "\n",
    "        hist_b_grads = [torch.zeros_like(bias).to(self.device) for bias in biases]\n",
    "        for i, b in enumerate(biases):\n",
    "            # 计算梯度\n",
    "            b_gradList = []\n",
    "            for state in range(n_states):\n",
    "                b_gradList.append(grad_for_state(state, b, grad1, reward))\n",
    "            # 计算梯度\n",
    "            b_grads = torch.stack(b_gradList)\n",
    "            b_grad = torch.sum(b_grads, dim=0)\n",
    "            # 清零历史梯度\n",
    "            if hist_b_grads[i] is not None:\n",
    "                hist_b_grads[i].zero_()\n",
    "            # 更新历史梯度平方累积和\n",
    "            hist_b_grads[i] += b_grad ** 2\n",
    "            # 计算调整后的梯度\n",
    "            adj_b_grad = b_grad / (adagrad_epsilon + torch.sqrt(hist_b_grads[i]))\n",
    "            # 添加参数更新规则到更新列表\n",
    "            updates.append((b, b + adj_b_grad * self.learning_rate))\n",
    "\n",
    "        n = int((len(updates) - 1) / 2)\n",
    "        with torch.no_grad():\n",
    "            for name, param in net[0].named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    param.data = updates[1][1]\n",
    "                elif 'bias' in name:\n",
    "                    param.data = updates[n+1][1]\n",
    "            \n",
    "            for name, param in net[1].named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    param.data = updates[2][1]\n",
    "                elif 'bias' in name:\n",
    "                    param.data = updates[n+2][1]\n",
    "                    \n",
    "        net.zero_grad()\n",
    "        rewardNew = torch.matmul(net(self.stateFeatures), updates[0][1]).reshape(n_states,)\n",
    "\n",
    "        # 2024.5.19 日更新：对 reward 进行标准化操作\n",
    "        rewardNew1 = (rewardNew.max()-rewardNew)/(rewardNew.max()-rewardNew.min())\n",
    "        \n",
    "        # ### 2024.5.18 9:25 此处更新，将所有不存在路段的网格奖励归 0\n",
    "        # rewardNew1 = torch.zeros_like(rewardNew)\n",
    "        # for loc in self.PassList:\n",
    "        #     rewardNew1[loc] = rewardNew[loc]\n",
    "            \n",
    "        return updates[0][1], net, rewardNew1\n",
    "\n",
    "    def train(self, n):\n",
    "        ini_net = self.ini_net\n",
    "        ############\n",
    "        ini_alpha = torch.normal(mean=0, std=0.01, size=(4, 1)).to(self.device)    # 此处的代码也需要适应神经网络的变化\n",
    "        # rewardList= []\n",
    "        # svfDiff = []\n",
    "        rewardList = []\n",
    "        stdList = []\n",
    "        num_epochs = self.epochs\n",
    "        for i in range(n):\n",
    "            with tqdm(total=int(num_epochs / n), desc='Iteration %d' % i) as pbar:\n",
    "                for i_epoch in range(int(num_epochs / n)):\n",
    "                    # 奖励的更新\n",
    "                    alpha, output_net, reward = self.update(net=ini_net, alpha=ini_alpha)\n",
    "                    ini_net = nn.Sequential(*list(output_net.children()))\n",
    "                    ini_alpha = alpha\n",
    "                    # rewardList.append(reward)\n",
    "                    # svfDiff.append(svf)\n",
    "                    rewardList.append(reward.mean())\n",
    "                    stdList.append(reward.std())\n",
    "\n",
    "                    if (i_epoch + 1) % n == 0:\n",
    "                        pbar.set_postfix({\n",
    "                            'epoch':\n",
    "                                '%d' % (num_epochs / n * i + i_epoch + 1),\n",
    "                            # 'return':\n",
    "                            # '%.3f' % np.mean(return_list[-10:])\n",
    "                        })\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "        return rewardList, stdList, reward, ini_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f96cbe-3c1c-408d-b68a-cfacc452849f",
   "metadata": {},
   "source": [
    "# Step 3 创建智能体运行的环境并训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0e869-6ddf-40d4-938c-d65ba7d1be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The Script Is Running······')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483d553-0a11-4ce8-ac42-733347176359",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrTable = pd.read_csv(r'./03_stateAttrNorm_output.txt', sep=',')\n",
    "attrTable1 = pd.read_csv(r'./NormAttrPass.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc822e8-7fac-40b3-8586-b0f5a6726f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'F:\\BaiduNetdiskDownload\\共享单车轨迹\\共享单车轨迹\\01_研究生毕业论文\\1_2_DataPreProcess')\n",
    "trajFile = r'SAPairCom.txt'\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "trajData = pd.read_csv(trajFile)  # ['FID', 'OrderID', 'TrajID', 'timeStamp', 'LG_ID', 'state', 'action', 'nextState']\n",
    "\n",
    "# Create Environment For Bicycle To Move\n",
    "env = bicycleGridRiding(attrTable=attrTable)\n",
    "state, _ = env.reset()\n",
    "print('Environment Is Successfully Built')\n",
    "\n",
    "trajProcess = TrajProccess(env=env, trajData=trajData, device=device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40310235-d1c6-47be-9f43-89eb24c1d065",
   "metadata": {},
   "source": [
    "2024.5.18 如果我只将存在道路的网格特征输入至神经网络中进行训练效果会如何？\n",
    "- 如果9：20的方法不奏效，则将上述方法也加入其中\n",
    "- 2024.5.18 21:36 只将存在道路的网格特征的数据输入至神经网络之中\n",
    "- 2024.5.20 奖励看起来收敛了，只不过训练轮次比较多，存在过拟合的情况。因此，需要加入 random.seed 之后看训练效果。训练大概需要 20 个轮次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249011e2-fe75-44f5-970c-3c65ed771b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "passListIndex = attrTable1['FID'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a304b1-1762-4456-b50d-222e714cdb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Reinforcement Learning\n",
    "# Training\n",
    "stateFeatures = torch.tensor(np.vstack([attrTable1[attrTable1['FID']==s].to_numpy()[0, 2:] for s in passListIndex]),\n",
    "                                 dtype=torch.float32).to(device)\n",
    "n_states, d_states = stateFeatures.shape    # numStates, dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d46b9f-2733-44de-95a2-c249ad96d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa316bf-cd39-48e0-a386-f75004de6c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_seed(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4c05a-3e1c-4117-8618-d5415ef2cb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Model Parameters\n",
    "net = nn.Sequential(nn.Linear(23, 12),\n",
    "                    nn.Linear(12, 6),\n",
    "                    nn.BatchNorm1d(6),\n",
    "                    nn.Softmax(),\n",
    "                    nn.Linear(6, 4),\n",
    "                    nn.BatchNorm1d(4),\n",
    "                    nn.Softmax())\n",
    "def init_weights(m):\n",
    "    '''\n",
    "    :param m: Neural Network Module\n",
    "    :return:\n",
    "    '''\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_normal_(m.weight)\n",
    "\n",
    "net.apply(init_weights)\n",
    "net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0b8f13-9628-4332-95a3-ceb529ed2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states, d_states\n",
    "ini_alpha = torch.normal(mean=0, std=0.01, size=(4, 1)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6c8392-513c-4664-81b2-0f722e9ff114",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_net = nn.Sequential(*list(net.children()))\n",
    "\n",
    "agent = deepIRL(env=env, \n",
    "                stateFeatures=stateFeatures, \n",
    "                trajProcess=trajProcess, \n",
    "                ini_net=ini_net, \n",
    "                discount=0.9, \n",
    "                device=device, \n",
    "                epochs=18,\n",
    "                passList=passListIndex)\n",
    "rewardList, stdList, reward, ini_alpha = agent.train(n=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d026330e-1008-4230-ab99-ef49f623db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewardList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa395872-ab78-4887-bd2e-b7de0e99efcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stdList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44624b-d93a-4bd9-a79f-7ebb7a4fb077",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9717c-8aaa-4a83-bc13-912ee33405c7",
   "metadata": {},
   "source": [
    "保存运算所得的结果以及其参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebecf3b3-e947-40ff-a6dd-6913ac56c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_alpha1 = ini_alpha.cpu().detach().numpy()\n",
    "np.save('Alpha_pass_Final', ini_alpha1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9526eb-8643-416f-a0c3-72e98995962b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, './NetParam_Pass_Final.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c9f79b-51cf-43ed-b734-4470106b6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_cpu = reward.cpu().detach().numpy()\n",
    "dataFrame = pd.DataFrame({'index':passListIndex, 'reward':reward_cpu})\n",
    "dataFrame.to_csv(r'./reward_Pass_Final.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e8d91-e023-4e42-b964-7dbd1ee6c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_List_cpu = rewardList.cpu().detach().numpy()\n",
    "# stdList_cpu = stdList.cpu().detach().numpy()\n",
    "df1 = pd.DataFrame({'index':[i for i in range(len(rewardList))], \n",
    "                    'reward':[rewardMean.cpu().detach().numpy() for rewardMean in rewardList], \n",
    "                    'std': [stdMean.cpu().detach().numpy() for stdMean in stdList]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc832835-7cb2-4f95-b4d3-1f973c792438",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a1eb4-7b18-465f-9f0f-47ad649aa23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(r'./rewardTrend.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70532e2e-ee82-44a1-9b64-d65d79279e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
